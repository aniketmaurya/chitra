{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"chitra What is chitra? chitra ( \u091a\u093f\u0924\u094d\u0930 ) is a multi-functional library for full-stack Deep Learning. It simplifies Model Building, API development, and Model Deployment. Components Load Image from Internet url, filepath or numpy array and plot Bounding Boxes on the images easily. Model Training and Explainable AI. Easily create UI for Machine Learning models or Rest API backend that can be deployed for serving ML Models in Production. \ud83d\udccc Highlights: [New] Auto Dockerization of Models \ud83d\udc33 [New] Framework Agnostic Model Serving & Interactive UI prototype app \u2728\ud83c\udf1f [New] Data Visualization, Bounding Box Visualization \ud83d\udc36\ud83c\udfa8 Model interpretation using GradCAM/GradCAM++ with no extra code \ud83d\udd25 Faster data loading without any boilerplate \ud83e\udd3a Progressive resizing of images \ud83c\udfa8 Rapid experiments with different models using chitra.trainer module \ud83d\ude80 \ud83d\ude98 Implementation Roadmap One click deployment to serverless platform. If you have more use case please raise an issue/PR with the feature you want. If you want to contribute, feel free to raise a PR. It doesn't need to be perfect. We will help you get there. \ud83d\udcc0 Installation Using pip (recommended) Minimum installation pip install -U chitra Full Installation pip install -U 'chitra[all]' Install for Training pip install -U 'chitra[nn]' Install for Serving pip install -U 'chitra[serve]' From source pip install git+https://github.com/aniketmaurya/chitra@master Or, git clone https://github.com/aniketmaurya/chitra.git cd chitra pip install . \ud83e\uddd1\u200d\ud83d\udcbb Usage Loading data for image classification Chitra dataloader and datagenerator modules for loading data. dataloader is a minimal dataloader that returns tf.data.Dataset object. datagenerator provides flexibility to users on how they want to load and manipulate the data. import numpy as np import chitra from chitra.dataloader import Clf import matplotlib.pyplot as plt clf_dl = Clf () data = clf_dl . from_folder ( cat_dog_path , target_shape = ( 224 , 224 )) clf_dl . show_batch ( 8 , figsize = ( 8 , 8 )) Image datagenerator Dataset class provides the flexibility to load image dataset by updating components of the class. Components of Dataset class are: image file generator resizer label generator image loader These components can be updated with custom function by the user according to their dataset structure. For example the Tiny Imagenet dataset is organized as- train_folder/ .....folder1/ .....file.txt .....folder2/ .....image1.jpg .....image2.jpg . . . ......imageN.jpg The inbuilt file generator search for images on the folder1 , now we can just update the image file generator and rest of the functionality will remain same. Dataset also support progressive resizing of images. Updating component from chitra.datagenerator import Dataset ds = Dataset ( data_path ) # it will load the folders and NOT images ds . filenames [: 3 ] Output No item present in the image size list ['/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/n02795169_boxes.txt', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02769748/images'] def load_files ( path ): return glob ( f ' { path } /*/images/*' ) def get_label ( path ): return path . split ( '/' )[ - 3 ] ds . update_component ( 'get_filenames' , load_files ) ds . filenames [: 3 ] Output get_filenames updated with No item present in the image size list ['/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_369.JPEG', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_386.JPEG', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_105.JPEG'] Progressive resizing It is the technique to sequentially resize all the images while training the CNNs on smaller to bigger image sizes. Progressive Resizing is described briefly in his terrific fastai course, \u201cPractical Deep Learning for Coders\u201d. A great way to use this technique is to train a model with smaller image size say 64x64, then use the weights of this model to train another model on images of size 128x128 and so on. Each larger-scale model incorporates the previous smaller-scale model layers and weights in its architecture. ~ KDnuggets image_sz_list = [( 28 , 28 ), ( 32 , 32 ), ( 64 , 64 )] ds = Dataset ( data_path , image_size = image_sz_list ) ds . update_component ( 'get_filenames' , load_files ) ds . update_component ( 'get_label' , get_label ) # first call to generator for img , label in ds . generator (): print ( 'first call to generator:' , img . shape ) break # seconds call to generator for img , label in ds . generator (): print ( 'seconds call to generator:' , img . shape ) break # third call to generator for img , label in ds . generator (): print ( 'third call to generator:' , img . shape ) break Output get_filenames updated with get_label updated with first call to generator: (28, 28, 3) seconds call to generator: (32, 32, 3) third call to generator: (64, 64, 3) tf.data support Creating a tf.data dataloader was never as easy as this one liner. It converts the Python generator into tf.data.Dataset for a faster data loading, prefetching, caching and everything provided by tf.data. image_sz_list = [( 28 , 28 ), ( 32 , 32 ), ( 64 , 64 )] ds = Dataset ( data_path , image_size = image_sz_list ) ds . update_component ( 'get_filenames' , load_files ) ds . update_component ( 'get_label' , get_label ) dl = ds . get_tf_dataset () for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) Output get_filenames updated with get_label updated with (28, 28, 3) (32, 32, 3) (64, 64, 3) Trainer The Trainer class inherits from tf.keras.Model , it contains everything that is required for training. It exposes trainer.cyclic_fit method which trains the model using Cyclic Learning rate discovered by Leslie Smith . from chitra.trainer import Trainer , create_cnn from chitra.datagenerator import Dataset ds = Dataset ( cat_dog_path , image_size = ( 224 , 224 )) model = create_cnn ( 'mobilenetv2' , num_classes = 2 , name = 'Cat_Dog_Model' ) trainer = Trainer ( ds , model ) # trainer.summary() trainer . compile2 ( batch_size = 8 , optimizer = tf . keras . optimizers . SGD ( 1e-3 , momentum = 0.9 , nesterov = True ), lr_range = ( 1e-6 , 1e-3 ), loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ]) trainer . cyclic_fit ( epochs = 5 , batch_size = 8 , lr_range = ( 0.00001 , 0.0001 ), ) Training Loop... cyclic learning rate already set! Epoch 1/5 1/1 [==============================] - 0s 14ms/step - loss: 6.4702 - binary_accuracy: 0.2500 Epoch 2/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 965us/step - loss: 5.9033 - binary_accuracy: 0.5000 Epoch 3/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 977us/step - loss: 5.9233 - binary_accuracy: 0.5000 Epoch 4/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 979us/step - loss: 2.1408 - binary_accuracy: 0.7500 Epoch 5/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 982us/step - loss: 1.9062 - binary_accuracy: 0.8750 \u2728 Model Interpretability It is important to understand what is going inside the model. Techniques like GradCam and Saliency Maps can visualize what the Network is learning. trainer module has InterpretModel class which creates GradCam and GradCam++ visualization with almost no additional code. from chitra.trainer import InterpretModel trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 1000 , keras_applications = False )) model_interpret = InterpretModel ( True , trainer ) image = ds [ 1 ][ 0 ] . numpy () . astype ( 'uint8' ) image = Image . fromarray ( image ) model_interpret ( image ) print ( IMAGENET_LABELS [ 285 ]) Returning the last set size which is: (224, 224) index: 282 Egyptian Mau \ud83c\udfa8 Data Visualization Image annotation Bounding Box creation is based on top of imgaug library. from chitra.image import Chitra import matplotlib.pyplot as plt bbox = [ 70 , 25 , 190 , 210 ] label = 'Dog' image = Chitra ( image_path , bboxes = bbox , labels = label ) plt . imshow ( image . draw_boxes ()) See Play with Images for detailed example! \ud83d\ude80 Model Serving (Framework Agnostic) Chitra can Create Rest API or Interactive UI app for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow, PyTorch or SKLearn. It provides chitra.serve.GradioApp for building Interactive UI app for ML/DL models and chitra.serve.API for building Rest API endpoint. from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) API Docs Preview ![Preview Model Server](https://raw.githubusercontent.com/aniketmaurya/chitra/master/docs/examples/model-server/preview.png) See Example Section for detailed explanation! \ud83d\udee0 Utility Limit GPU memory or enable dynamic GPU memory growth for Tensorflow. from chitra.utility.tf_utils import limit_gpu , gpu_dynamic_mem_growth # limit the amount of GPU required for your training limit_gpu ( gpu_id = 0 , memory_limit = 1024 * 2 ) No GPU:0 found in your system! gpu_dynamic_mem_growth () No GPU found on the machine! \ud83e\udd17 Contribute Contributions of any kind are welcome. Please check the Contributing Guidelines before contributing. Code Of Conduct We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Read full Contributor Covenant Code of Conduct Acknowledgement chitra is built with help of awesome libraries like Tensorflow 2.x , imgaug , FastAPI and Gradio .","title":"Introduction"},{"location":"#chitra","text":"","title":"chitra"},{"location":"#what-is-chitra","text":"chitra ( \u091a\u093f\u0924\u094d\u0930 ) is a multi-functional library for full-stack Deep Learning. It simplifies Model Building, API development, and Model Deployment.","title":"What is chitra?"},{"location":"#components","text":"Load Image from Internet url, filepath or numpy array and plot Bounding Boxes on the images easily. Model Training and Explainable AI. Easily create UI for Machine Learning models or Rest API backend that can be deployed for serving ML Models in Production.","title":"Components"},{"location":"#highlights","text":"[New] Auto Dockerization of Models \ud83d\udc33 [New] Framework Agnostic Model Serving & Interactive UI prototype app \u2728\ud83c\udf1f [New] Data Visualization, Bounding Box Visualization \ud83d\udc36\ud83c\udfa8 Model interpretation using GradCAM/GradCAM++ with no extra code \ud83d\udd25 Faster data loading without any boilerplate \ud83e\udd3a Progressive resizing of images \ud83c\udfa8 Rapid experiments with different models using chitra.trainer module \ud83d\ude80","title":"\ud83d\udccc Highlights:"},{"location":"#implementation-roadmap","text":"One click deployment to serverless platform. If you have more use case please raise an issue/PR with the feature you want. If you want to contribute, feel free to raise a PR. It doesn't need to be perfect. We will help you get there.","title":"\ud83d\ude98 Implementation Roadmap"},{"location":"#installation","text":"","title":"\ud83d\udcc0 Installation"},{"location":"#using-pip-recommended","text":"Minimum installation pip install -U chitra Full Installation pip install -U 'chitra[all]' Install for Training pip install -U 'chitra[nn]' Install for Serving pip install -U 'chitra[serve]'","title":"Using pip (recommended)"},{"location":"#from-source","text":"pip install git+https://github.com/aniketmaurya/chitra@master Or, git clone https://github.com/aniketmaurya/chitra.git cd chitra pip install .","title":"From source"},{"location":"#usage","text":"","title":"\ud83e\uddd1\u200d\ud83d\udcbb Usage"},{"location":"#loading-data-for-image-classification","text":"Chitra dataloader and datagenerator modules for loading data. dataloader is a minimal dataloader that returns tf.data.Dataset object. datagenerator provides flexibility to users on how they want to load and manipulate the data. import numpy as np import chitra from chitra.dataloader import Clf import matplotlib.pyplot as plt clf_dl = Clf () data = clf_dl . from_folder ( cat_dog_path , target_shape = ( 224 , 224 )) clf_dl . show_batch ( 8 , figsize = ( 8 , 8 ))","title":"Loading data for image classification"},{"location":"#image-datagenerator","text":"Dataset class provides the flexibility to load image dataset by updating components of the class. Components of Dataset class are: image file generator resizer label generator image loader These components can be updated with custom function by the user according to their dataset structure. For example the Tiny Imagenet dataset is organized as- train_folder/ .....folder1/ .....file.txt .....folder2/ .....image1.jpg .....image2.jpg . . . ......imageN.jpg The inbuilt file generator search for images on the folder1 , now we can just update the image file generator and rest of the functionality will remain same. Dataset also support progressive resizing of images.","title":"Image datagenerator"},{"location":"#updating-component","text":"from chitra.datagenerator import Dataset ds = Dataset ( data_path ) # it will load the folders and NOT images ds . filenames [: 3 ] Output No item present in the image size list ['/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/n02795169_boxes.txt', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02769748/images'] def load_files ( path ): return glob ( f ' { path } /*/images/*' ) def get_label ( path ): return path . split ( '/' )[ - 3 ] ds . update_component ( 'get_filenames' , load_files ) ds . filenames [: 3 ] Output get_filenames updated with No item present in the image size list ['/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_369.JPEG', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_386.JPEG', '/Users/aniket/Pictures/data/tiny-imagenet-200/train/n02795169/images/n02795169_105.JPEG']","title":"Updating component"},{"location":"#progressive-resizing","text":"It is the technique to sequentially resize all the images while training the CNNs on smaller to bigger image sizes. Progressive Resizing is described briefly in his terrific fastai course, \u201cPractical Deep Learning for Coders\u201d. A great way to use this technique is to train a model with smaller image size say 64x64, then use the weights of this model to train another model on images of size 128x128 and so on. Each larger-scale model incorporates the previous smaller-scale model layers and weights in its architecture. ~ KDnuggets image_sz_list = [( 28 , 28 ), ( 32 , 32 ), ( 64 , 64 )] ds = Dataset ( data_path , image_size = image_sz_list ) ds . update_component ( 'get_filenames' , load_files ) ds . update_component ( 'get_label' , get_label ) # first call to generator for img , label in ds . generator (): print ( 'first call to generator:' , img . shape ) break # seconds call to generator for img , label in ds . generator (): print ( 'seconds call to generator:' , img . shape ) break # third call to generator for img , label in ds . generator (): print ( 'third call to generator:' , img . shape ) break Output get_filenames updated with get_label updated with first call to generator: (28, 28, 3) seconds call to generator: (32, 32, 3) third call to generator: (64, 64, 3)","title":"Progressive resizing"},{"location":"#tfdata-support","text":"Creating a tf.data dataloader was never as easy as this one liner. It converts the Python generator into tf.data.Dataset for a faster data loading, prefetching, caching and everything provided by tf.data. image_sz_list = [( 28 , 28 ), ( 32 , 32 ), ( 64 , 64 )] ds = Dataset ( data_path , image_size = image_sz_list ) ds . update_component ( 'get_filenames' , load_files ) ds . update_component ( 'get_label' , get_label ) dl = ds . get_tf_dataset () for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) for e in dl . take ( 1 ): print ( e [ 0 ] . shape ) Output get_filenames updated with get_label updated with (28, 28, 3) (32, 32, 3) (64, 64, 3)","title":"tf.data support"},{"location":"#trainer","text":"The Trainer class inherits from tf.keras.Model , it contains everything that is required for training. It exposes trainer.cyclic_fit method which trains the model using Cyclic Learning rate discovered by Leslie Smith . from chitra.trainer import Trainer , create_cnn from chitra.datagenerator import Dataset ds = Dataset ( cat_dog_path , image_size = ( 224 , 224 )) model = create_cnn ( 'mobilenetv2' , num_classes = 2 , name = 'Cat_Dog_Model' ) trainer = Trainer ( ds , model ) # trainer.summary() trainer . compile2 ( batch_size = 8 , optimizer = tf . keras . optimizers . SGD ( 1e-3 , momentum = 0.9 , nesterov = True ), lr_range = ( 1e-6 , 1e-3 ), loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ]) trainer . cyclic_fit ( epochs = 5 , batch_size = 8 , lr_range = ( 0.00001 , 0.0001 ), ) Training Loop... cyclic learning rate already set! Epoch 1/5 1/1 [==============================] - 0s 14ms/step - loss: 6.4702 - binary_accuracy: 0.2500 Epoch 2/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 965us/step - loss: 5.9033 - binary_accuracy: 0.5000 Epoch 3/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 977us/step - loss: 5.9233 - binary_accuracy: 0.5000 Epoch 4/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 979us/step - loss: 2.1408 - binary_accuracy: 0.7500 Epoch 5/5 Returning the last set size which is: (224, 224) 1/1 [==============================] - 0s 982us/step - loss: 1.9062 - binary_accuracy: 0.8750","title":"Trainer"},{"location":"#model-interpretability","text":"It is important to understand what is going inside the model. Techniques like GradCam and Saliency Maps can visualize what the Network is learning. trainer module has InterpretModel class which creates GradCam and GradCam++ visualization with almost no additional code. from chitra.trainer import InterpretModel trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 1000 , keras_applications = False )) model_interpret = InterpretModel ( True , trainer ) image = ds [ 1 ][ 0 ] . numpy () . astype ( 'uint8' ) image = Image . fromarray ( image ) model_interpret ( image ) print ( IMAGENET_LABELS [ 285 ]) Returning the last set size which is: (224, 224) index: 282 Egyptian Mau","title":"\u2728 Model Interpretability"},{"location":"#data-visualization","text":"","title":"\ud83c\udfa8 Data Visualization"},{"location":"#image-annotation","text":"Bounding Box creation is based on top of imgaug library. from chitra.image import Chitra import matplotlib.pyplot as plt bbox = [ 70 , 25 , 190 , 210 ] label = 'Dog' image = Chitra ( image_path , bboxes = bbox , labels = label ) plt . imshow ( image . draw_boxes ()) See Play with Images for detailed example!","title":"Image annotation"},{"location":"#model-serving-framework-agnostic","text":"Chitra can Create Rest API or Interactive UI app for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow, PyTorch or SKLearn. It provides chitra.serve.GradioApp for building Interactive UI app for ML/DL models and chitra.serve.API for building Rest API endpoint. from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) API Docs Preview ![Preview Model Server](https://raw.githubusercontent.com/aniketmaurya/chitra/master/docs/examples/model-server/preview.png) See Example Section for detailed explanation!","title":"\ud83d\ude80 Model Serving (Framework Agnostic)"},{"location":"#utility","text":"Limit GPU memory or enable dynamic GPU memory growth for Tensorflow. from chitra.utility.tf_utils import limit_gpu , gpu_dynamic_mem_growth # limit the amount of GPU required for your training limit_gpu ( gpu_id = 0 , memory_limit = 1024 * 2 ) No GPU:0 found in your system! gpu_dynamic_mem_growth () No GPU found on the machine!","title":"\ud83d\udee0 Utility"},{"location":"#contribute","text":"Contributions of any kind are welcome. Please check the Contributing Guidelines before contributing.","title":"\ud83e\udd17 Contribute"},{"location":"#code-of-conduct","text":"We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Read full Contributor Covenant Code of Conduct","title":"Code Of Conduct"},{"location":"#acknowledgement","text":"chitra is built with help of awesome libraries like Tensorflow 2.x , imgaug , FastAPI and Gradio .","title":"Acknowledgement"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2020-21 Aniket Maurya Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"examples/chitra-class/chitra-class/","text":"Play with Images Chitra is an image utility class that can load image from filelike object, web url or numpy image. It offers drawing bounding box over the image. # pip install -U chitra from chitra.image import Chitra import matplotlib.pyplot as plt What can it do? Load image from file, filelike object , web url, or numpy array Plot image Plot bounding boxes along with labels in no extra code. Specify bounding box format: Center(xywh): center x,y and height width of bbox Corner(xyxy): xmin ymin and xmax ymax Plot bounding box on image Load image from web url and show url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png\" image = Chitra ( url ) image . imshow () You can cache the image downloaded from internet URL by passing cache=True in argument. Second call to the same URL will not download from internet, instead image will be loaded from the local cache dir. # first call - image will be downloaded from internet and saved to local cache dir image = Chitra ( url , cache = True ) # second call - image will be loaded from local cached dir image = Chitra ( url , cache = True ) Plot bounding box and label for the handphone box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . image = image . image . convert ( 'RGB' ) plt . imshow ( image . draw_boxes ()) Resize Image and Bounding at the same time Chitra can rescale your bounding box automatically based on the new image size. box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . resize_image_with_bbox (( 224 , 224 )) print ( image . bounding_boxes ) plt . imshow ( image . draw_boxes ())","title":"Play with Images"},{"location":"examples/chitra-class/chitra-class/#play-with-images","text":"Chitra is an image utility class that can load image from filelike object, web url or numpy image. It offers drawing bounding box over the image. # pip install -U chitra from chitra.image import Chitra import matplotlib.pyplot as plt","title":"Play with Images"},{"location":"examples/chitra-class/chitra-class/#what-can-it-do","text":"Load image from file, filelike object , web url, or numpy array Plot image Plot bounding boxes along with labels in no extra code. Specify bounding box format: Center(xywh): center x,y and height width of bbox Corner(xyxy): xmin ymin and xmax ymax Plot bounding box on image","title":"What can it do?"},{"location":"examples/chitra-class/chitra-class/#load-image-from-web-url-and-show","text":"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png\" image = Chitra ( url ) image . imshow () You can cache the image downloaded from internet URL by passing cache=True in argument. Second call to the same URL will not download from internet, instead image will be loaded from the local cache dir. # first call - image will be downloaded from internet and saved to local cache dir image = Chitra ( url , cache = True ) # second call - image will be loaded from local cached dir image = Chitra ( url , cache = True )","title":"Load image from web url and show"},{"location":"examples/chitra-class/chitra-class/#plot-bounding-box-and-label-for-the-handphone","text":"box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . image = image . image . convert ( 'RGB' ) plt . imshow ( image . draw_boxes ())","title":"Plot bounding box and label for the handphone"},{"location":"examples/chitra-class/chitra-class/#resize-image-and-bounding-at-the-same-time","text":"Chitra can rescale your bounding box automatically based on the new image size. box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . resize_image_with_bbox (( 224 , 224 )) print ( image . bounding_boxes ) plt . imshow ( image . draw_boxes ())","title":"Resize Image and Bounding at the same time"},{"location":"examples/image-classification/image-classification/","text":"Training Image classifier with Chitra Training Image classification model for Cats vs Dogs Kaggle dataset. To install chitra pip install --upgrade \"chitra[nn]\" import functions and classes Dataset Class Dataset class has API for loading tf.data , image augmentation and progressive resizing. Trainer The Trainer class inherits from tf.keras.Model, it contains everything that is required for training. It exposes trainer.cyclic_fit method which trains the model using Cyclic Learning rate discovered by Leslie Smith. import tensorflow as tf from chitra.datagenerator import Dataset from chitra.trainer import Trainer , create_cnn from PIL import Image BS = 16 IMG_SIZE_LST = [( 128 , 128 ), ( 160 , 160 ), ( 224 , 224 )] AUTOTUNE = tf . data . experimental . AUTOTUNE def tensor_to_image ( tensor ): return Image . fromarray ( tensor . numpy () . astype ( 'uint8' )) Copy your kaggle key to /root/.kaggle/kaggle.json for downloading the dataset. !kaggle datasets download -d chetankv/dogs-cats-images !unzip -q dogs-cats-images.zip ds = Dataset ( 'dog vs cat/dataset/training_set' , image_size = IMG_SIZE_LST ) image , label = ds [ 0 ] print ( label ) tensor_to_image ( image ) . resize (( 224 , 224 )) dogs Create Trainer Train imagenet pretrained MobileNetV2 model with cyclic learning rate and SGD optimizer. trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 2 )) WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. trainer . summary () Model Summary Model: \"functional_1\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, None, None, 0 __________________________________________________________________________________________________ Conv1_pad (ZeroPadding2D) (None, None, None, 3 0 input_1[0][0] __________________________________________________________________________________________________ Conv1 (Conv2D) (None, None, None, 3 864 Conv1_pad[0][0] __________________________________________________________________________________________________ bn_Conv1 (BatchNormalization) (None, None, None, 3 128 Conv1[0][0] __________________________________________________________________________________________________ Conv1_relu (ReLU) (None, None, None, 3 0 bn_Conv1[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise (Depthw (None, None, None, 3 288 Conv1_relu[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_BN (Bat (None, None, None, 3 128 expanded_conv_depthwise[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_relu (R (None, None, None, 3 0 expanded_conv_depthwise_BN[0][0] __________________________________________________________________________________________________ expanded_conv_project (Conv2D) (None, None, None, 1 512 expanded_conv_depthwise_relu[0][0 __________________________________________________________________________________________________ expanded_conv_project_BN (Batch (None, None, None, 1 64 expanded_conv_project[0][0] __________________________________________________________________________________________________ block_1_expand (Conv2D) (None, None, None, 9 1536 expanded_conv_project_BN[0][0] __________________________________________________________________________________________________ block_1_expand_BN (BatchNormali (None, None, None, 9 384 block_1_expand[0][0] __________________________________________________________________________________________________ block_1_expand_relu (ReLU) (None, None, None, 9 0 block_1_expand_BN[0][0] __________________________________________________________________________________________________ block_1_pad (ZeroPadding2D) (None, None, None, 9 0 block_1_expand_relu[0][0] __________________________________________________________________________________________________ block_1_depthwise (DepthwiseCon (None, None, None, 9 864 block_1_pad[0][0] __________________________________________________________________________________________________ block_1_depthwise_BN (BatchNorm (None, None, None, 9 384 block_1_depthwise[0][0] __________________________________________________________________________________________________ block_1_depthwise_relu (ReLU) (None, None, None, 9 0 block_1_depthwise_BN[0][0] __________________________________________________________________________________________________ block_1_project (Conv2D) (None, None, None, 2 2304 block_1_depthwise_relu[0][0] __________________________________________________________________________________________________ block_1_project_BN (BatchNormal (None, None, None, 2 96 block_1_project[0][0] __________________________________________________________________________________________________ block_2_expand (Conv2D) (None, None, None, 1 3456 block_1_project_BN[0][0] __________________________________________________________________________________________________ block_2_expand_BN (BatchNormali (None, None, None, 1 576 block_2_expand[0][0] __________________________________________________________________________________________________ block_2_expand_relu (ReLU) (None, None, None, 1 0 block_2_expand_BN[0][0] __________________________________________________________________________________________________ block_2_depthwise (DepthwiseCon (None, None, None, 1 1296 block_2_expand_relu[0][0] __________________________________________________________________________________________________ block_2_depthwise_BN (BatchNorm (None, None, None, 1 576 block_2_depthwise[0][0] __________________________________________________________________________________________________ block_2_depthwise_relu (ReLU) (None, None, None, 1 0 block_2_depthwise_BN[0][0] __________________________________________________________________________________________________ block_2_project (Conv2D) (None, None, None, 2 3456 block_2_depthwise_relu[0][0] __________________________________________________________________________________________________ block_2_project_BN (BatchNormal (None, None, None, 2 96 block_2_project[0][0] __________________________________________________________________________________________________ block_2_add (Add) (None, None, None, 2 0 block_1_project_BN[0][0] block_2_project_BN[0][0] __________________________________________________________________________________________________ block_3_expand (Conv2D) (None, None, None, 1 3456 block_2_add[0][0] __________________________________________________________________________________________________ block_3_expand_BN (BatchNormali (None, None, None, 1 576 block_3_expand[0][0] __________________________________________________________________________________________________ block_3_expand_relu (ReLU) (None, None, None, 1 0 block_3_expand_BN[0][0] __________________________________________________________________________________________________ block_3_pad (ZeroPadding2D) (None, None, None, 1 0 block_3_expand_relu[0][0] __________________________________________________________________________________________________ block_3_depthwise (DepthwiseCon (None, None, None, 1 1296 block_3_pad[0][0] __________________________________________________________________________________________________ block_3_depthwise_BN (BatchNorm (None, None, None, 1 576 block_3_depthwise[0][0] __________________________________________________________________________________________________ block_3_depthwise_relu (ReLU) (None, None, None, 1 0 block_3_depthwise_BN[0][0] __________________________________________________________________________________________________ block_3_project (Conv2D) (None, None, None, 3 4608 block_3_depthwise_relu[0][0] __________________________________________________________________________________________________ block_3_project_BN (BatchNormal (None, None, None, 3 128 block_3_project[0][0] __________________________________________________________________________________________________ block_4_expand (Conv2D) (None, None, None, 1 6144 block_3_project_BN[0][0] __________________________________________________________________________________________________ block_4_expand_BN (BatchNormali (None, None, None, 1 768 block_4_expand[0][0] __________________________________________________________________________________________________ block_4_expand_relu (ReLU) (None, None, None, 1 0 block_4_expand_BN[0][0] __________________________________________________________________________________________________ block_4_depthwise (DepthwiseCon (None, None, None, 1 1728 block_4_expand_relu[0][0] __________________________________________________________________________________________________ block_4_depthwise_BN (BatchNorm (None, None, None, 1 768 block_4_depthwise[0][0] __________________________________________________________________________________________________ block_4_depthwise_relu (ReLU) (None, None, None, 1 0 block_4_depthwise_BN[0][0] __________________________________________________________________________________________________ block_4_project (Conv2D) (None, None, None, 3 6144 block_4_depthwise_relu[0][0] __________________________________________________________________________________________________ block_4_project_BN (BatchNormal (None, None, None, 3 128 block_4_project[0][0] __________________________________________________________________________________________________ block_4_add (Add) (None, None, None, 3 0 block_3_project_BN[0][0] block_4_project_BN[0][0] __________________________________________________________________________________________________ block_5_expand (Conv2D) (None, None, None, 1 6144 block_4_add[0][0] __________________________________________________________________________________________________ block_5_expand_BN (BatchNormali (None, None, None, 1 768 block_5_expand[0][0] __________________________________________________________________________________________________ block_5_expand_relu (ReLU) (None, None, None, 1 0 block_5_expand_BN[0][0] __________________________________________________________________________________________________ block_5_depthwise (DepthwiseCon (None, None, None, 1 1728 block_5_expand_relu[0][0] __________________________________________________________________________________________________ block_5_depthwise_BN (BatchNorm (None, None, None, 1 768 block_5_depthwise[0][0] __________________________________________________________________________________________________ block_5_depthwise_relu (ReLU) (None, None, None, 1 0 block_5_depthwise_BN[0][0] __________________________________________________________________________________________________ block_5_project (Conv2D) (None, None, None, 3 6144 block_5_depthwise_relu[0][0] __________________________________________________________________________________________________ block_5_project_BN (BatchNormal (None, None, None, 3 128 block_5_project[0][0] __________________________________________________________________________________________________ block_5_add (Add) (None, None, None, 3 0 block_4_add[0][0] block_5_project_BN[0][0] __________________________________________________________________________________________________ block_6_expand (Conv2D) (None, None, None, 1 6144 block_5_add[0][0] __________________________________________________________________________________________________ block_6_expand_BN (BatchNormali (None, None, None, 1 768 block_6_expand[0][0] __________________________________________________________________________________________________ block_6_expand_relu (ReLU) (None, None, None, 1 0 block_6_expand_BN[0][0] __________________________________________________________________________________________________ block_6_pad (ZeroPadding2D) (None, None, None, 1 0 block_6_expand_relu[0][0] __________________________________________________________________________________________________ block_6_depthwise (DepthwiseCon (None, None, None, 1 1728 block_6_pad[0][0] __________________________________________________________________________________________________ block_6_depthwise_BN (BatchNorm (None, None, None, 1 768 block_6_depthwise[0][0] __________________________________________________________________________________________________ block_6_depthwise_relu (ReLU) (None, None, None, 1 0 block_6_depthwise_BN[0][0] __________________________________________________________________________________________________ block_6_project (Conv2D) (None, None, None, 6 12288 block_6_depthwise_relu[0][0] __________________________________________________________________________________________________ block_6_project_BN (BatchNormal (None, None, None, 6 256 block_6_project[0][0] __________________________________________________________________________________________________ block_7_expand (Conv2D) (None, None, None, 3 24576 block_6_project_BN[0][0] __________________________________________________________________________________________________ block_7_expand_BN (BatchNormali (None, None, None, 3 1536 block_7_expand[0][0] __________________________________________________________________________________________________ block_7_expand_relu (ReLU) (None, None, None, 3 0 block_7_expand_BN[0][0] __________________________________________________________________________________________________ block_7_depthwise (DepthwiseCon (None, None, None, 3 3456 block_7_expand_relu[0][0] __________________________________________________________________________________________________ block_7_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_7_depthwise[0][0] __________________________________________________________________________________________________ block_7_depthwise_relu (ReLU) (None, None, None, 3 0 block_7_depthwise_BN[0][0] __________________________________________________________________________________________________ block_7_project (Conv2D) (None, None, None, 6 24576 block_7_depthwise_relu[0][0] __________________________________________________________________________________________________ block_7_project_BN (BatchNormal (None, None, None, 6 256 block_7_project[0][0] __________________________________________________________________________________________________ block_7_add (Add) (None, None, None, 6 0 block_6_project_BN[0][0] block_7_project_BN[0][0] __________________________________________________________________________________________________ block_8_expand (Conv2D) (None, None, None, 3 24576 block_7_add[0][0] __________________________________________________________________________________________________ block_8_expand_BN (BatchNormali (None, None, None, 3 1536 block_8_expand[0][0] __________________________________________________________________________________________________ block_8_expand_relu (ReLU) (None, None, None, 3 0 block_8_expand_BN[0][0] __________________________________________________________________________________________________ block_8_depthwise (DepthwiseCon (None, None, None, 3 3456 block_8_expand_relu[0][0] __________________________________________________________________________________________________ block_8_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_8_depthwise[0][0] __________________________________________________________________________________________________ block_8_depthwise_relu (ReLU) (None, None, None, 3 0 block_8_depthwise_BN[0][0] __________________________________________________________________________________________________ block_8_project (Conv2D) (None, None, None, 6 24576 block_8_depthwise_relu[0][0] __________________________________________________________________________________________________ block_8_project_BN (BatchNormal (None, None, None, 6 256 block_8_project[0][0] __________________________________________________________________________________________________ block_8_add (Add) (None, None, None, 6 0 block_7_add[0][0] block_8_project_BN[0][0] __________________________________________________________________________________________________ block_9_expand (Conv2D) (None, None, None, 3 24576 block_8_add[0][0] __________________________________________________________________________________________________ block_9_expand_BN (BatchNormali (None, None, None, 3 1536 block_9_expand[0][0] __________________________________________________________________________________________________ block_9_expand_relu (ReLU) (None, None, None, 3 0 block_9_expand_BN[0][0] __________________________________________________________________________________________________ block_9_depthwise (DepthwiseCon (None, None, None, 3 3456 block_9_expand_relu[0][0] __________________________________________________________________________________________________ block_9_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_9_depthwise[0][0] __________________________________________________________________________________________________ block_9_depthwise_relu (ReLU) (None, None, None, 3 0 block_9_depthwise_BN[0][0] __________________________________________________________________________________________________ block_9_project (Conv2D) (None, None, None, 6 24576 block_9_depthwise_relu[0][0] __________________________________________________________________________________________________ block_9_project_BN (BatchNormal (None, None, None, 6 256 block_9_project[0][0] __________________________________________________________________________________________________ block_9_add (Add) (None, None, None, 6 0 block_8_add[0][0] block_9_project_BN[0][0] __________________________________________________________________________________________________ block_10_expand (Conv2D) (None, None, None, 3 24576 block_9_add[0][0] __________________________________________________________________________________________________ block_10_expand_BN (BatchNormal (None, None, None, 3 1536 block_10_expand[0][0] __________________________________________________________________________________________________ block_10_expand_relu (ReLU) (None, None, None, 3 0 block_10_expand_BN[0][0] __________________________________________________________________________________________________ block_10_depthwise (DepthwiseCo (None, None, None, 3 3456 block_10_expand_relu[0][0] __________________________________________________________________________________________________ block_10_depthwise_BN (BatchNor (None, None, None, 3 1536 block_10_depthwise[0][0] __________________________________________________________________________________________________ block_10_depthwise_relu (ReLU) (None, None, None, 3 0 block_10_depthwise_BN[0][0] __________________________________________________________________________________________________ block_10_project (Conv2D) (None, None, None, 9 36864 block_10_depthwise_relu[0][0] __________________________________________________________________________________________________ block_10_project_BN (BatchNorma (None, None, None, 9 384 block_10_project[0][0] __________________________________________________________________________________________________ block_11_expand (Conv2D) (None, None, None, 5 55296 block_10_project_BN[0][0] __________________________________________________________________________________________________ block_11_expand_BN (BatchNormal (None, None, None, 5 2304 block_11_expand[0][0] __________________________________________________________________________________________________ block_11_expand_relu (ReLU) (None, None, None, 5 0 block_11_expand_BN[0][0] __________________________________________________________________________________________________ block_11_depthwise (DepthwiseCo (None, None, None, 5 5184 block_11_expand_relu[0][0] __________________________________________________________________________________________________ block_11_depthwise_BN (BatchNor (None, None, None, 5 2304 block_11_depthwise[0][0] __________________________________________________________________________________________________ block_11_depthwise_relu (ReLU) (None, None, None, 5 0 block_11_depthwise_BN[0][0] __________________________________________________________________________________________________ block_11_project (Conv2D) (None, None, None, 9 55296 block_11_depthwise_relu[0][0] __________________________________________________________________________________________________ block_11_project_BN (BatchNorma (None, None, None, 9 384 block_11_project[0][0] __________________________________________________________________________________________________ block_11_add (Add) (None, None, None, 9 0 block_10_project_BN[0][0] block_11_project_BN[0][0] __________________________________________________________________________________________________ block_12_expand (Conv2D) (None, None, None, 5 55296 block_11_add[0][0] __________________________________________________________________________________________________ block_12_expand_BN (BatchNormal (None, None, None, 5 2304 block_12_expand[0][0] __________________________________________________________________________________________________ block_12_expand_relu (ReLU) (None, None, None, 5 0 block_12_expand_BN[0][0] __________________________________________________________________________________________________ block_12_depthwise (DepthwiseCo (None, None, None, 5 5184 block_12_expand_relu[0][0] __________________________________________________________________________________________________ block_12_depthwise_BN (BatchNor (None, None, None, 5 2304 block_12_depthwise[0][0] __________________________________________________________________________________________________ block_12_depthwise_relu (ReLU) (None, None, None, 5 0 block_12_depthwise_BN[0][0] __________________________________________________________________________________________________ block_12_project (Conv2D) (None, None, None, 9 55296 block_12_depthwise_relu[0][0] __________________________________________________________________________________________________ block_12_project_BN (BatchNorma (None, None, None, 9 384 block_12_project[0][0] __________________________________________________________________________________________________ block_12_add (Add) (None, None, None, 9 0 block_11_add[0][0] block_12_project_BN[0][0] __________________________________________________________________________________________________ block_13_expand (Conv2D) (None, None, None, 5 55296 block_12_add[0][0] __________________________________________________________________________________________________ block_13_expand_BN (BatchNormal (None, None, None, 5 2304 block_13_expand[0][0] __________________________________________________________________________________________________ block_13_expand_relu (ReLU) (None, None, None, 5 0 block_13_expand_BN[0][0] __________________________________________________________________________________________________ block_13_pad (ZeroPadding2D) (None, None, None, 5 0 block_13_expand_relu[0][0] __________________________________________________________________________________________________ block_13_depthwise (DepthwiseCo (None, None, None, 5 5184 block_13_pad[0][0] __________________________________________________________________________________________________ block_13_depthwise_BN (BatchNor (None, None, None, 5 2304 block_13_depthwise[0][0] __________________________________________________________________________________________________ block_13_depthwise_relu (ReLU) (None, None, None, 5 0 block_13_depthwise_BN[0][0] __________________________________________________________________________________________________ block_13_project (Conv2D) (None, None, None, 1 92160 block_13_depthwise_relu[0][0] __________________________________________________________________________________________________ block_13_project_BN (BatchNorma (None, None, None, 1 640 block_13_project[0][0] __________________________________________________________________________________________________ block_14_expand (Conv2D) (None, None, None, 9 153600 block_13_project_BN[0][0] __________________________________________________________________________________________________ block_14_expand_BN (BatchNormal (None, None, None, 9 3840 block_14_expand[0][0] __________________________________________________________________________________________________ block_14_expand_relu (ReLU) (None, None, None, 9 0 block_14_expand_BN[0][0] __________________________________________________________________________________________________ block_14_depthwise (DepthwiseCo (None, None, None, 9 8640 block_14_expand_relu[0][0] __________________________________________________________________________________________________ block_14_depthwise_BN (BatchNor (None, None, None, 9 3840 block_14_depthwise[0][0] __________________________________________________________________________________________________ block_14_depthwise_relu (ReLU) (None, None, None, 9 0 block_14_depthwise_BN[0][0] __________________________________________________________________________________________________ block_14_project (Conv2D) (None, None, None, 1 153600 block_14_depthwise_relu[0][0] __________________________________________________________________________________________________ block_14_project_BN (BatchNorma (None, None, None, 1 640 block_14_project[0][0] __________________________________________________________________________________________________ block_14_add (Add) (None, None, None, 1 0 block_13_project_BN[0][0] block_14_project_BN[0][0] __________________________________________________________________________________________________ block_15_expand (Conv2D) (None, None, None, 9 153600 block_14_add[0][0] __________________________________________________________________________________________________ block_15_expand_BN (BatchNormal (None, None, None, 9 3840 block_15_expand[0][0] __________________________________________________________________________________________________ block_15_expand_relu (ReLU) (None, None, None, 9 0 block_15_expand_BN[0][0] __________________________________________________________________________________________________ block_15_depthwise (DepthwiseCo (None, None, None, 9 8640 block_15_expand_relu[0][0] __________________________________________________________________________________________________ block_15_depthwise_BN (BatchNor (None, None, None, 9 3840 block_15_depthwise[0][0] __________________________________________________________________________________________________ block_15_depthwise_relu (ReLU) (None, None, None, 9 0 block_15_depthwise_BN[0][0] __________________________________________________________________________________________________ block_15_project (Conv2D) (None, None, None, 1 153600 block_15_depthwise_relu[0][0] __________________________________________________________________________________________________ block_15_project_BN (BatchNorma (None, None, None, 1 640 block_15_project[0][0] __________________________________________________________________________________________________ block_15_add (Add) (None, None, None, 1 0 block_14_add[0][0] block_15_project_BN[0][0] __________________________________________________________________________________________________ block_16_expand (Conv2D) (None, None, None, 9 153600 block_15_add[0][0] __________________________________________________________________________________________________ block_16_expand_BN (BatchNormal (None, None, None, 9 3840 block_16_expand[0][0] __________________________________________________________________________________________________ block_16_expand_relu (ReLU) (None, None, None, 9 0 block_16_expand_BN[0][0] __________________________________________________________________________________________________ block_16_depthwise (DepthwiseCo (None, None, None, 9 8640 block_16_expand_relu[0][0] __________________________________________________________________________________________________ block_16_depthwise_BN (BatchNor (None, None, None, 9 3840 block_16_depthwise[0][0] __________________________________________________________________________________________________ block_16_depthwise_relu (ReLU) (None, None, None, 9 0 block_16_depthwise_BN[0][0] __________________________________________________________________________________________________ block_16_project (Conv2D) (None, None, None, 3 307200 block_16_depthwise_relu[0][0] __________________________________________________________________________________________________ block_16_project_BN (BatchNorma (None, None, None, 3 1280 block_16_project[0][0] __________________________________________________________________________________________________ Conv_1 (Conv2D) (None, None, None, 1 409600 block_16_project_BN[0][0] __________________________________________________________________________________________________ Conv_1_bn (BatchNormalization) (None, None, None, 1 5120 Conv_1[0][0] __________________________________________________________________________________________________ out_relu (ReLU) (None, None, None, 1 0 Conv_1_bn[0][0] __________________________________________________________________________________________________ global_average_pooling2d (Globa (None, 1280) 0 out_relu[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 1280) 0 global_average_pooling2d[0][0] __________________________________________________________________________________________________ output (Dense) (None, 1) 1281 dropout[0][0] ================================================================================================== Total params: 2,259,265 Trainable params: 2,225,153 Non-trainable params: 34,112 __________________________________________________________________________________________________ trainer . compile2 ( batch_size = BS , optimizer = 'sgd' , lr_range = ( 1e-4 , 1e-2 ), loss = tf . keras . losses . BinaryCrossentropy ( from_logits = True ), metrics = [ 'binary_accuracy' ]) Model compiled! trainer . cyclic_fit ( 10 , batch_size = BS ) cyclic learning rate already set! Epoch 1/10 500/500 [==============================] - 40s 80ms/step - loss: 0.4258 - binary_accuracy: 0.7878 Epoch 2/10 500/500 [==============================] - 50s 101ms/step - loss: 0.1384 - binary_accuracy: 0.9438 Epoch 3/10 500/500 [==============================] - 79s 159ms/step - loss: 0.0587 - binary_accuracy: 0.9771 Epoch 4/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0385 - binary_accuracy: 0.9841 Epoch 5/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0257 - binary_accuracy: 0.9911 Epoch 6/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0302 - binary_accuracy: 0.9901 Epoch 7/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0212 - binary_accuracy: 0.9931 Epoch 8/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 157ms/step - loss: 0.0207 - binary_accuracy: 0.9935 Epoch 9/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0177 - binary_accuracy: 0.9951 Epoch 10/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 159ms/step - loss: 0.0172 - binary_accuracy: 0.9940 <tensorflow.python.keras.callbacks.History at 0x7f67581730b8> Trainer also supports the regular keras model.fit api using trainer.fit Train the same model without cyclic learning rate : trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 2 )) trainer . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy ( from_logits = True ), metrics = [ 'binary_accuracy' ]) WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. data = ds . get_tf_dataset () . map (( lambda x , y : ( x / 127.5 - 1.0 , y )), AUTOTUNE ) . batch ( BS ) . prefetch ( AUTOTUNE ) trainer . fit ( data , epochs = 10 ) Training loop... Epoch 1/10 500/500 [==============================] - 38s 77ms/step - loss: 0.4070 - binary_accuracy: 0.8026 Epoch 2/10 500/500 [==============================] - 50s 99ms/step - loss: 0.1800 - binary_accuracy: 0.9239 Epoch 3/10 500/500 [==============================] - 78s 155ms/step - loss: 0.1197 - binary_accuracy: 0.9553 Epoch 4/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0952 - binary_accuracy: 0.9626 Epoch 5/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 157ms/step - loss: 0.0809 - binary_accuracy: 0.9664 Epoch 6/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 77s 154ms/step - loss: 0.0693 - binary_accuracy: 0.9735 Epoch 7/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 156ms/step - loss: 0.0610 - binary_accuracy: 0.9759 Epoch 8/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 157ms/step - loss: 0.0530 - binary_accuracy: 0.9797 Epoch 9/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0505 - binary_accuracy: 0.9821 Epoch 10/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 156ms/step - loss: 0.0452 - binary_accuracy: 0.9829 <tensorflow.python.keras.callbacks.History at 0x7f662f0af1d0> What does model focus on while making a prediction? chitra.trainer.InterpretModel class creates GradCAM and GradCAM++ visualization in no additional code! from chitra.trainer import InterpretModel import random model_interpret = InterpretModel ( True , trainer ) image_tensor = random . choice ( ds )[ 0 ] image = tensor_to_image ( image_tensor ) model_interpret ( image , auto_resize = False )","title":"Image Classification"},{"location":"examples/image-classification/image-classification/#training-image-classifier-with-chitra","text":"Training Image classification model for Cats vs Dogs Kaggle dataset. To install chitra pip install --upgrade \"chitra[nn]\"","title":"Training Image classifier with Chitra"},{"location":"examples/image-classification/image-classification/#import-functions-and-classes","text":"","title":"import functions and classes"},{"location":"examples/image-classification/image-classification/#dataset-class","text":"Dataset class has API for loading tf.data , image augmentation and progressive resizing.","title":"Dataset Class"},{"location":"examples/image-classification/image-classification/#trainer","text":"The Trainer class inherits from tf.keras.Model, it contains everything that is required for training. It exposes trainer.cyclic_fit method which trains the model using Cyclic Learning rate discovered by Leslie Smith. import tensorflow as tf from chitra.datagenerator import Dataset from chitra.trainer import Trainer , create_cnn from PIL import Image BS = 16 IMG_SIZE_LST = [( 128 , 128 ), ( 160 , 160 ), ( 224 , 224 )] AUTOTUNE = tf . data . experimental . AUTOTUNE def tensor_to_image ( tensor ): return Image . fromarray ( tensor . numpy () . astype ( 'uint8' )) Copy your kaggle key to /root/.kaggle/kaggle.json for downloading the dataset. !kaggle datasets download -d chetankv/dogs-cats-images !unzip -q dogs-cats-images.zip ds = Dataset ( 'dog vs cat/dataset/training_set' , image_size = IMG_SIZE_LST ) image , label = ds [ 0 ] print ( label ) tensor_to_image ( image ) . resize (( 224 , 224 )) dogs","title":"Trainer"},{"location":"examples/image-classification/image-classification/#create-trainer","text":"Train imagenet pretrained MobileNetV2 model with cyclic learning rate and SGD optimizer. trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 2 )) WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. trainer . summary () Model Summary Model: \"functional_1\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, None, None, 0 __________________________________________________________________________________________________ Conv1_pad (ZeroPadding2D) (None, None, None, 3 0 input_1[0][0] __________________________________________________________________________________________________ Conv1 (Conv2D) (None, None, None, 3 864 Conv1_pad[0][0] __________________________________________________________________________________________________ bn_Conv1 (BatchNormalization) (None, None, None, 3 128 Conv1[0][0] __________________________________________________________________________________________________ Conv1_relu (ReLU) (None, None, None, 3 0 bn_Conv1[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise (Depthw (None, None, None, 3 288 Conv1_relu[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_BN (Bat (None, None, None, 3 128 expanded_conv_depthwise[0][0] __________________________________________________________________________________________________ expanded_conv_depthwise_relu (R (None, None, None, 3 0 expanded_conv_depthwise_BN[0][0] __________________________________________________________________________________________________ expanded_conv_project (Conv2D) (None, None, None, 1 512 expanded_conv_depthwise_relu[0][0 __________________________________________________________________________________________________ expanded_conv_project_BN (Batch (None, None, None, 1 64 expanded_conv_project[0][0] __________________________________________________________________________________________________ block_1_expand (Conv2D) (None, None, None, 9 1536 expanded_conv_project_BN[0][0] __________________________________________________________________________________________________ block_1_expand_BN (BatchNormali (None, None, None, 9 384 block_1_expand[0][0] __________________________________________________________________________________________________ block_1_expand_relu (ReLU) (None, None, None, 9 0 block_1_expand_BN[0][0] __________________________________________________________________________________________________ block_1_pad (ZeroPadding2D) (None, None, None, 9 0 block_1_expand_relu[0][0] __________________________________________________________________________________________________ block_1_depthwise (DepthwiseCon (None, None, None, 9 864 block_1_pad[0][0] __________________________________________________________________________________________________ block_1_depthwise_BN (BatchNorm (None, None, None, 9 384 block_1_depthwise[0][0] __________________________________________________________________________________________________ block_1_depthwise_relu (ReLU) (None, None, None, 9 0 block_1_depthwise_BN[0][0] __________________________________________________________________________________________________ block_1_project (Conv2D) (None, None, None, 2 2304 block_1_depthwise_relu[0][0] __________________________________________________________________________________________________ block_1_project_BN (BatchNormal (None, None, None, 2 96 block_1_project[0][0] __________________________________________________________________________________________________ block_2_expand (Conv2D) (None, None, None, 1 3456 block_1_project_BN[0][0] __________________________________________________________________________________________________ block_2_expand_BN (BatchNormali (None, None, None, 1 576 block_2_expand[0][0] __________________________________________________________________________________________________ block_2_expand_relu (ReLU) (None, None, None, 1 0 block_2_expand_BN[0][0] __________________________________________________________________________________________________ block_2_depthwise (DepthwiseCon (None, None, None, 1 1296 block_2_expand_relu[0][0] __________________________________________________________________________________________________ block_2_depthwise_BN (BatchNorm (None, None, None, 1 576 block_2_depthwise[0][0] __________________________________________________________________________________________________ block_2_depthwise_relu (ReLU) (None, None, None, 1 0 block_2_depthwise_BN[0][0] __________________________________________________________________________________________________ block_2_project (Conv2D) (None, None, None, 2 3456 block_2_depthwise_relu[0][0] __________________________________________________________________________________________________ block_2_project_BN (BatchNormal (None, None, None, 2 96 block_2_project[0][0] __________________________________________________________________________________________________ block_2_add (Add) (None, None, None, 2 0 block_1_project_BN[0][0] block_2_project_BN[0][0] __________________________________________________________________________________________________ block_3_expand (Conv2D) (None, None, None, 1 3456 block_2_add[0][0] __________________________________________________________________________________________________ block_3_expand_BN (BatchNormali (None, None, None, 1 576 block_3_expand[0][0] __________________________________________________________________________________________________ block_3_expand_relu (ReLU) (None, None, None, 1 0 block_3_expand_BN[0][0] __________________________________________________________________________________________________ block_3_pad (ZeroPadding2D) (None, None, None, 1 0 block_3_expand_relu[0][0] __________________________________________________________________________________________________ block_3_depthwise (DepthwiseCon (None, None, None, 1 1296 block_3_pad[0][0] __________________________________________________________________________________________________ block_3_depthwise_BN (BatchNorm (None, None, None, 1 576 block_3_depthwise[0][0] __________________________________________________________________________________________________ block_3_depthwise_relu (ReLU) (None, None, None, 1 0 block_3_depthwise_BN[0][0] __________________________________________________________________________________________________ block_3_project (Conv2D) (None, None, None, 3 4608 block_3_depthwise_relu[0][0] __________________________________________________________________________________________________ block_3_project_BN (BatchNormal (None, None, None, 3 128 block_3_project[0][0] __________________________________________________________________________________________________ block_4_expand (Conv2D) (None, None, None, 1 6144 block_3_project_BN[0][0] __________________________________________________________________________________________________ block_4_expand_BN (BatchNormali (None, None, None, 1 768 block_4_expand[0][0] __________________________________________________________________________________________________ block_4_expand_relu (ReLU) (None, None, None, 1 0 block_4_expand_BN[0][0] __________________________________________________________________________________________________ block_4_depthwise (DepthwiseCon (None, None, None, 1 1728 block_4_expand_relu[0][0] __________________________________________________________________________________________________ block_4_depthwise_BN (BatchNorm (None, None, None, 1 768 block_4_depthwise[0][0] __________________________________________________________________________________________________ block_4_depthwise_relu (ReLU) (None, None, None, 1 0 block_4_depthwise_BN[0][0] __________________________________________________________________________________________________ block_4_project (Conv2D) (None, None, None, 3 6144 block_4_depthwise_relu[0][0] __________________________________________________________________________________________________ block_4_project_BN (BatchNormal (None, None, None, 3 128 block_4_project[0][0] __________________________________________________________________________________________________ block_4_add (Add) (None, None, None, 3 0 block_3_project_BN[0][0] block_4_project_BN[0][0] __________________________________________________________________________________________________ block_5_expand (Conv2D) (None, None, None, 1 6144 block_4_add[0][0] __________________________________________________________________________________________________ block_5_expand_BN (BatchNormali (None, None, None, 1 768 block_5_expand[0][0] __________________________________________________________________________________________________ block_5_expand_relu (ReLU) (None, None, None, 1 0 block_5_expand_BN[0][0] __________________________________________________________________________________________________ block_5_depthwise (DepthwiseCon (None, None, None, 1 1728 block_5_expand_relu[0][0] __________________________________________________________________________________________________ block_5_depthwise_BN (BatchNorm (None, None, None, 1 768 block_5_depthwise[0][0] __________________________________________________________________________________________________ block_5_depthwise_relu (ReLU) (None, None, None, 1 0 block_5_depthwise_BN[0][0] __________________________________________________________________________________________________ block_5_project (Conv2D) (None, None, None, 3 6144 block_5_depthwise_relu[0][0] __________________________________________________________________________________________________ block_5_project_BN (BatchNormal (None, None, None, 3 128 block_5_project[0][0] __________________________________________________________________________________________________ block_5_add (Add) (None, None, None, 3 0 block_4_add[0][0] block_5_project_BN[0][0] __________________________________________________________________________________________________ block_6_expand (Conv2D) (None, None, None, 1 6144 block_5_add[0][0] __________________________________________________________________________________________________ block_6_expand_BN (BatchNormali (None, None, None, 1 768 block_6_expand[0][0] __________________________________________________________________________________________________ block_6_expand_relu (ReLU) (None, None, None, 1 0 block_6_expand_BN[0][0] __________________________________________________________________________________________________ block_6_pad (ZeroPadding2D) (None, None, None, 1 0 block_6_expand_relu[0][0] __________________________________________________________________________________________________ block_6_depthwise (DepthwiseCon (None, None, None, 1 1728 block_6_pad[0][0] __________________________________________________________________________________________________ block_6_depthwise_BN (BatchNorm (None, None, None, 1 768 block_6_depthwise[0][0] __________________________________________________________________________________________________ block_6_depthwise_relu (ReLU) (None, None, None, 1 0 block_6_depthwise_BN[0][0] __________________________________________________________________________________________________ block_6_project (Conv2D) (None, None, None, 6 12288 block_6_depthwise_relu[0][0] __________________________________________________________________________________________________ block_6_project_BN (BatchNormal (None, None, None, 6 256 block_6_project[0][0] __________________________________________________________________________________________________ block_7_expand (Conv2D) (None, None, None, 3 24576 block_6_project_BN[0][0] __________________________________________________________________________________________________ block_7_expand_BN (BatchNormali (None, None, None, 3 1536 block_7_expand[0][0] __________________________________________________________________________________________________ block_7_expand_relu (ReLU) (None, None, None, 3 0 block_7_expand_BN[0][0] __________________________________________________________________________________________________ block_7_depthwise (DepthwiseCon (None, None, None, 3 3456 block_7_expand_relu[0][0] __________________________________________________________________________________________________ block_7_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_7_depthwise[0][0] __________________________________________________________________________________________________ block_7_depthwise_relu (ReLU) (None, None, None, 3 0 block_7_depthwise_BN[0][0] __________________________________________________________________________________________________ block_7_project (Conv2D) (None, None, None, 6 24576 block_7_depthwise_relu[0][0] __________________________________________________________________________________________________ block_7_project_BN (BatchNormal (None, None, None, 6 256 block_7_project[0][0] __________________________________________________________________________________________________ block_7_add (Add) (None, None, None, 6 0 block_6_project_BN[0][0] block_7_project_BN[0][0] __________________________________________________________________________________________________ block_8_expand (Conv2D) (None, None, None, 3 24576 block_7_add[0][0] __________________________________________________________________________________________________ block_8_expand_BN (BatchNormali (None, None, None, 3 1536 block_8_expand[0][0] __________________________________________________________________________________________________ block_8_expand_relu (ReLU) (None, None, None, 3 0 block_8_expand_BN[0][0] __________________________________________________________________________________________________ block_8_depthwise (DepthwiseCon (None, None, None, 3 3456 block_8_expand_relu[0][0] __________________________________________________________________________________________________ block_8_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_8_depthwise[0][0] __________________________________________________________________________________________________ block_8_depthwise_relu (ReLU) (None, None, None, 3 0 block_8_depthwise_BN[0][0] __________________________________________________________________________________________________ block_8_project (Conv2D) (None, None, None, 6 24576 block_8_depthwise_relu[0][0] __________________________________________________________________________________________________ block_8_project_BN (BatchNormal (None, None, None, 6 256 block_8_project[0][0] __________________________________________________________________________________________________ block_8_add (Add) (None, None, None, 6 0 block_7_add[0][0] block_8_project_BN[0][0] __________________________________________________________________________________________________ block_9_expand (Conv2D) (None, None, None, 3 24576 block_8_add[0][0] __________________________________________________________________________________________________ block_9_expand_BN (BatchNormali (None, None, None, 3 1536 block_9_expand[0][0] __________________________________________________________________________________________________ block_9_expand_relu (ReLU) (None, None, None, 3 0 block_9_expand_BN[0][0] __________________________________________________________________________________________________ block_9_depthwise (DepthwiseCon (None, None, None, 3 3456 block_9_expand_relu[0][0] __________________________________________________________________________________________________ block_9_depthwise_BN (BatchNorm (None, None, None, 3 1536 block_9_depthwise[0][0] __________________________________________________________________________________________________ block_9_depthwise_relu (ReLU) (None, None, None, 3 0 block_9_depthwise_BN[0][0] __________________________________________________________________________________________________ block_9_project (Conv2D) (None, None, None, 6 24576 block_9_depthwise_relu[0][0] __________________________________________________________________________________________________ block_9_project_BN (BatchNormal (None, None, None, 6 256 block_9_project[0][0] __________________________________________________________________________________________________ block_9_add (Add) (None, None, None, 6 0 block_8_add[0][0] block_9_project_BN[0][0] __________________________________________________________________________________________________ block_10_expand (Conv2D) (None, None, None, 3 24576 block_9_add[0][0] __________________________________________________________________________________________________ block_10_expand_BN (BatchNormal (None, None, None, 3 1536 block_10_expand[0][0] __________________________________________________________________________________________________ block_10_expand_relu (ReLU) (None, None, None, 3 0 block_10_expand_BN[0][0] __________________________________________________________________________________________________ block_10_depthwise (DepthwiseCo (None, None, None, 3 3456 block_10_expand_relu[0][0] __________________________________________________________________________________________________ block_10_depthwise_BN (BatchNor (None, None, None, 3 1536 block_10_depthwise[0][0] __________________________________________________________________________________________________ block_10_depthwise_relu (ReLU) (None, None, None, 3 0 block_10_depthwise_BN[0][0] __________________________________________________________________________________________________ block_10_project (Conv2D) (None, None, None, 9 36864 block_10_depthwise_relu[0][0] __________________________________________________________________________________________________ block_10_project_BN (BatchNorma (None, None, None, 9 384 block_10_project[0][0] __________________________________________________________________________________________________ block_11_expand (Conv2D) (None, None, None, 5 55296 block_10_project_BN[0][0] __________________________________________________________________________________________________ block_11_expand_BN (BatchNormal (None, None, None, 5 2304 block_11_expand[0][0] __________________________________________________________________________________________________ block_11_expand_relu (ReLU) (None, None, None, 5 0 block_11_expand_BN[0][0] __________________________________________________________________________________________________ block_11_depthwise (DepthwiseCo (None, None, None, 5 5184 block_11_expand_relu[0][0] __________________________________________________________________________________________________ block_11_depthwise_BN (BatchNor (None, None, None, 5 2304 block_11_depthwise[0][0] __________________________________________________________________________________________________ block_11_depthwise_relu (ReLU) (None, None, None, 5 0 block_11_depthwise_BN[0][0] __________________________________________________________________________________________________ block_11_project (Conv2D) (None, None, None, 9 55296 block_11_depthwise_relu[0][0] __________________________________________________________________________________________________ block_11_project_BN (BatchNorma (None, None, None, 9 384 block_11_project[0][0] __________________________________________________________________________________________________ block_11_add (Add) (None, None, None, 9 0 block_10_project_BN[0][0] block_11_project_BN[0][0] __________________________________________________________________________________________________ block_12_expand (Conv2D) (None, None, None, 5 55296 block_11_add[0][0] __________________________________________________________________________________________________ block_12_expand_BN (BatchNormal (None, None, None, 5 2304 block_12_expand[0][0] __________________________________________________________________________________________________ block_12_expand_relu (ReLU) (None, None, None, 5 0 block_12_expand_BN[0][0] __________________________________________________________________________________________________ block_12_depthwise (DepthwiseCo (None, None, None, 5 5184 block_12_expand_relu[0][0] __________________________________________________________________________________________________ block_12_depthwise_BN (BatchNor (None, None, None, 5 2304 block_12_depthwise[0][0] __________________________________________________________________________________________________ block_12_depthwise_relu (ReLU) (None, None, None, 5 0 block_12_depthwise_BN[0][0] __________________________________________________________________________________________________ block_12_project (Conv2D) (None, None, None, 9 55296 block_12_depthwise_relu[0][0] __________________________________________________________________________________________________ block_12_project_BN (BatchNorma (None, None, None, 9 384 block_12_project[0][0] __________________________________________________________________________________________________ block_12_add (Add) (None, None, None, 9 0 block_11_add[0][0] block_12_project_BN[0][0] __________________________________________________________________________________________________ block_13_expand (Conv2D) (None, None, None, 5 55296 block_12_add[0][0] __________________________________________________________________________________________________ block_13_expand_BN (BatchNormal (None, None, None, 5 2304 block_13_expand[0][0] __________________________________________________________________________________________________ block_13_expand_relu (ReLU) (None, None, None, 5 0 block_13_expand_BN[0][0] __________________________________________________________________________________________________ block_13_pad (ZeroPadding2D) (None, None, None, 5 0 block_13_expand_relu[0][0] __________________________________________________________________________________________________ block_13_depthwise (DepthwiseCo (None, None, None, 5 5184 block_13_pad[0][0] __________________________________________________________________________________________________ block_13_depthwise_BN (BatchNor (None, None, None, 5 2304 block_13_depthwise[0][0] __________________________________________________________________________________________________ block_13_depthwise_relu (ReLU) (None, None, None, 5 0 block_13_depthwise_BN[0][0] __________________________________________________________________________________________________ block_13_project (Conv2D) (None, None, None, 1 92160 block_13_depthwise_relu[0][0] __________________________________________________________________________________________________ block_13_project_BN (BatchNorma (None, None, None, 1 640 block_13_project[0][0] __________________________________________________________________________________________________ block_14_expand (Conv2D) (None, None, None, 9 153600 block_13_project_BN[0][0] __________________________________________________________________________________________________ block_14_expand_BN (BatchNormal (None, None, None, 9 3840 block_14_expand[0][0] __________________________________________________________________________________________________ block_14_expand_relu (ReLU) (None, None, None, 9 0 block_14_expand_BN[0][0] __________________________________________________________________________________________________ block_14_depthwise (DepthwiseCo (None, None, None, 9 8640 block_14_expand_relu[0][0] __________________________________________________________________________________________________ block_14_depthwise_BN (BatchNor (None, None, None, 9 3840 block_14_depthwise[0][0] __________________________________________________________________________________________________ block_14_depthwise_relu (ReLU) (None, None, None, 9 0 block_14_depthwise_BN[0][0] __________________________________________________________________________________________________ block_14_project (Conv2D) (None, None, None, 1 153600 block_14_depthwise_relu[0][0] __________________________________________________________________________________________________ block_14_project_BN (BatchNorma (None, None, None, 1 640 block_14_project[0][0] __________________________________________________________________________________________________ block_14_add (Add) (None, None, None, 1 0 block_13_project_BN[0][0] block_14_project_BN[0][0] __________________________________________________________________________________________________ block_15_expand (Conv2D) (None, None, None, 9 153600 block_14_add[0][0] __________________________________________________________________________________________________ block_15_expand_BN (BatchNormal (None, None, None, 9 3840 block_15_expand[0][0] __________________________________________________________________________________________________ block_15_expand_relu (ReLU) (None, None, None, 9 0 block_15_expand_BN[0][0] __________________________________________________________________________________________________ block_15_depthwise (DepthwiseCo (None, None, None, 9 8640 block_15_expand_relu[0][0] __________________________________________________________________________________________________ block_15_depthwise_BN (BatchNor (None, None, None, 9 3840 block_15_depthwise[0][0] __________________________________________________________________________________________________ block_15_depthwise_relu (ReLU) (None, None, None, 9 0 block_15_depthwise_BN[0][0] __________________________________________________________________________________________________ block_15_project (Conv2D) (None, None, None, 1 153600 block_15_depthwise_relu[0][0] __________________________________________________________________________________________________ block_15_project_BN (BatchNorma (None, None, None, 1 640 block_15_project[0][0] __________________________________________________________________________________________________ block_15_add (Add) (None, None, None, 1 0 block_14_add[0][0] block_15_project_BN[0][0] __________________________________________________________________________________________________ block_16_expand (Conv2D) (None, None, None, 9 153600 block_15_add[0][0] __________________________________________________________________________________________________ block_16_expand_BN (BatchNormal (None, None, None, 9 3840 block_16_expand[0][0] __________________________________________________________________________________________________ block_16_expand_relu (ReLU) (None, None, None, 9 0 block_16_expand_BN[0][0] __________________________________________________________________________________________________ block_16_depthwise (DepthwiseCo (None, None, None, 9 8640 block_16_expand_relu[0][0] __________________________________________________________________________________________________ block_16_depthwise_BN (BatchNor (None, None, None, 9 3840 block_16_depthwise[0][0] __________________________________________________________________________________________________ block_16_depthwise_relu (ReLU) (None, None, None, 9 0 block_16_depthwise_BN[0][0] __________________________________________________________________________________________________ block_16_project (Conv2D) (None, None, None, 3 307200 block_16_depthwise_relu[0][0] __________________________________________________________________________________________________ block_16_project_BN (BatchNorma (None, None, None, 3 1280 block_16_project[0][0] __________________________________________________________________________________________________ Conv_1 (Conv2D) (None, None, None, 1 409600 block_16_project_BN[0][0] __________________________________________________________________________________________________ Conv_1_bn (BatchNormalization) (None, None, None, 1 5120 Conv_1[0][0] __________________________________________________________________________________________________ out_relu (ReLU) (None, None, None, 1 0 Conv_1_bn[0][0] __________________________________________________________________________________________________ global_average_pooling2d (Globa (None, 1280) 0 out_relu[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 1280) 0 global_average_pooling2d[0][0] __________________________________________________________________________________________________ output (Dense) (None, 1) 1281 dropout[0][0] ================================================================================================== Total params: 2,259,265 Trainable params: 2,225,153 Non-trainable params: 34,112 __________________________________________________________________________________________________ trainer . compile2 ( batch_size = BS , optimizer = 'sgd' , lr_range = ( 1e-4 , 1e-2 ), loss = tf . keras . losses . BinaryCrossentropy ( from_logits = True ), metrics = [ 'binary_accuracy' ]) Model compiled! trainer . cyclic_fit ( 10 , batch_size = BS ) cyclic learning rate already set! Epoch 1/10 500/500 [==============================] - 40s 80ms/step - loss: 0.4258 - binary_accuracy: 0.7878 Epoch 2/10 500/500 [==============================] - 50s 101ms/step - loss: 0.1384 - binary_accuracy: 0.9438 Epoch 3/10 500/500 [==============================] - 79s 159ms/step - loss: 0.0587 - binary_accuracy: 0.9771 Epoch 4/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0385 - binary_accuracy: 0.9841 Epoch 5/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0257 - binary_accuracy: 0.9911 Epoch 6/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0302 - binary_accuracy: 0.9901 Epoch 7/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0212 - binary_accuracy: 0.9931 Epoch 8/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 157ms/step - loss: 0.0207 - binary_accuracy: 0.9935 Epoch 9/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0177 - binary_accuracy: 0.9951 Epoch 10/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 159ms/step - loss: 0.0172 - binary_accuracy: 0.9940 <tensorflow.python.keras.callbacks.History at 0x7f67581730b8> Trainer also supports the regular keras model.fit api using trainer.fit Train the same model without cyclic learning rate : trainer = Trainer ( ds , create_cnn ( 'mobilenetv2' , num_classes = 2 )) trainer . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy ( from_logits = True ), metrics = [ 'binary_accuracy' ]) WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default. data = ds . get_tf_dataset () . map (( lambda x , y : ( x / 127.5 - 1.0 , y )), AUTOTUNE ) . batch ( BS ) . prefetch ( AUTOTUNE ) trainer . fit ( data , epochs = 10 ) Training loop... Epoch 1/10 500/500 [==============================] - 38s 77ms/step - loss: 0.4070 - binary_accuracy: 0.8026 Epoch 2/10 500/500 [==============================] - 50s 99ms/step - loss: 0.1800 - binary_accuracy: 0.9239 Epoch 3/10 500/500 [==============================] - 78s 155ms/step - loss: 0.1197 - binary_accuracy: 0.9553 Epoch 4/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0952 - binary_accuracy: 0.9626 Epoch 5/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 157ms/step - loss: 0.0809 - binary_accuracy: 0.9664 Epoch 6/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 77s 154ms/step - loss: 0.0693 - binary_accuracy: 0.9735 Epoch 7/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 156ms/step - loss: 0.0610 - binary_accuracy: 0.9759 Epoch 8/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 157ms/step - loss: 0.0530 - binary_accuracy: 0.9797 Epoch 9/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 79s 158ms/step - loss: 0.0505 - binary_accuracy: 0.9821 Epoch 10/10 Returning the last set size which is: (224, 224) 500/500 [==============================] - 78s 156ms/step - loss: 0.0452 - binary_accuracy: 0.9829 <tensorflow.python.keras.callbacks.History at 0x7f662f0af1d0>","title":"Create Trainer"},{"location":"examples/image-classification/image-classification/#what-does-model-focus-on-while-making-a-prediction","text":"chitra.trainer.InterpretModel class creates GradCAM and GradCAM++ visualization in no additional code! from chitra.trainer import InterpretModel import random model_interpret = InterpretModel ( True , trainer ) image_tensor = random . choice ( ds )[ 0 ] image = tensor_to_image ( image_tensor ) model_interpret ( image , auto_resize = False )","title":"What does model focus on while making a prediction?"},{"location":"examples/model-server/model-server/","text":"Chitra Model Server Create API for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow or PyTorch. What can it do? Serve Any Learning Model Predefined processing functions for image classification (NLP processing functions coming soon) Override custom preprocessing and Postprocessing function with your own. Code # install chitra # pip install -U chitra from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) Open http://127.0.0.1:8000/docs in your browser and try out the API. You can upload any image to try out the API. If you want to try out Text Classification or Question-Answering task then all you have to do is change api-type=\"text-classification\" or api_type=\"question-ans\" then pass your model and you are all set. Request Response Schema (JSON body) will be changed based on the api_type . Preview Question Answering API","title":"Model Server"},{"location":"examples/model-server/model-server/#chitra-model-server","text":"Create API for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow or PyTorch.","title":"Chitra Model Server"},{"location":"examples/model-server/model-server/#what-can-it-do","text":"Serve Any Learning Model Predefined processing functions for image classification (NLP processing functions coming soon) Override custom preprocessing and Postprocessing function with your own.","title":"What can it do?"},{"location":"examples/model-server/model-server/#code","text":"# install chitra # pip install -U chitra from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) Open http://127.0.0.1:8000/docs in your browser and try out the API. You can upload any image to try out the API. If you want to try out Text Classification or Question-Answering task then all you have to do is change api-type=\"text-classification\" or api_type=\"question-ans\" then pass your model and you are all set. Request Response Schema (JSON body) will be changed based on the api_type .","title":"Code"},{"location":"examples/model-server/model-server/#preview-question-answering-api","text":"","title":"Preview Question Answering API"},{"location":"source/api/image/chitra-class/","text":"Play with Images and Bounding Boxes Chitra is an image utility class that can load image from filelike object, web url or numpy image. It offers drawing bounding box over the image. # pip install -U chitra from chitra.image import Chitra import matplotlib.pyplot as plt What can it do? Load image from file, filelike object , web url, or numpy array Plot image Plot bounding boxes along with labels in no extra code. Specify bounding box format: Center(xywh): center x,y and height width of bbox Corner(xyxy): xmin ymin and xmax ymax Plot bounding box on image Resize Bounding Boxes with image resize Load image from web url and show url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png\" image = Chitra ( url ) image . imshow () You can cache the image downloaded from internet URL by passing cache=True in argument. Second call to the same URL will not download from internet, instead image will be loaded from the local cache dir. # first call - image will be downloaded from internet and saved to local cache dir image = Chitra ( url , cache = True ) # second call - image will be loaded from local cached dir image = Chitra ( url , cache = True ) Plot bounding box and label for the handphone box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . image = image . image . convert ( 'RGB' ) plt . imshow ( image . draw_boxes ()) Resize Image and Bounding at the same time Chitra can rescale your bounding box automatically based on the new image size. box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . resize_image_with_bbox (( 224 , 224 )) print ( image . bounding_boxes ) plt . imshow ( image . draw_boxes ())","title":"Image & Bounding Boxes"},{"location":"source/api/image/chitra-class/#play-with-images-and-bounding-boxes","text":"Chitra is an image utility class that can load image from filelike object, web url or numpy image. It offers drawing bounding box over the image. # pip install -U chitra from chitra.image import Chitra import matplotlib.pyplot as plt","title":"Play with Images and Bounding Boxes"},{"location":"source/api/image/chitra-class/#what-can-it-do","text":"Load image from file, filelike object , web url, or numpy array Plot image Plot bounding boxes along with labels in no extra code. Specify bounding box format: Center(xywh): center x,y and height width of bbox Corner(xyxy): xmin ymin and xmax ymax Plot bounding box on image Resize Bounding Boxes with image resize","title":"What can it do?"},{"location":"source/api/image/chitra-class/#load-image-from-web-url-and-show","text":"url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png\" image = Chitra ( url ) image . imshow () You can cache the image downloaded from internet URL by passing cache=True in argument. Second call to the same URL will not download from internet, instead image will be loaded from the local cache dir. # first call - image will be downloaded from internet and saved to local cache dir image = Chitra ( url , cache = True ) # second call - image will be loaded from local cached dir image = Chitra ( url , cache = True )","title":"Load image from web url and show"},{"location":"source/api/image/chitra-class/#plot-bounding-box-and-label-for-the-handphone","text":"box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . image = image . image . convert ( 'RGB' ) plt . imshow ( image . draw_boxes ())","title":"Plot bounding box and label for the handphone"},{"location":"source/api/image/chitra-class/#resize-image-and-bounding-at-the-same-time","text":"Chitra can rescale your bounding box automatically based on the new image size. box = [[ 600 , 250 , 900 , 600.1 ]] label = [ 'handphone' ] image = Chitra ( url , box , label ) image . resize_image_with_bbox (( 224 , 224 )) print ( image . bounding_boxes ) plt . imshow ( image . draw_boxes ())","title":"Resize Image and Bounding at the same time"},{"location":"source/api/serve/model_server/","text":"Serving ML Models with API or UI app Create Rest API or Interactive UI app for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow, PyTorch or SKLearn. What can it do? Create Rest API endpoint for Model Serving Create Interactive UI for Model Prototype Demo Share UI Demo with everyone by generating public url Predefined processing functions for image classification (NLP processing functions coming soon) Override custom preprocessing and Postprocessing function with your own. Request Response Schema (JSON body) will be changed based on the api_type . install: pip install -U \"chitra[serve]\" Default available API types are: Image Classification Object Detection Text Classification Question Answering To get a full list of available API types you can call chitra.serve.API.get_available_api_types() . Create Rest API Text Classification API You can easily create Sentiment Analysis API. In this example, I will use HuggingFace to load the Sentiment Analysis Model but feel free to use other models as well. from transformers import AutoModelForSequenceClassification , AutoTokenizer , pipeline from chitra.serve import create_api tokenizer = AutoTokenizer . from_pretrained ( \"finiteautomata/beto-sentiment-analysis\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"finiteautomata/beto-sentiment-analysis\" ) classifier = pipeline ( \"sentiment-analysis\" , model = model , tokenizer = tokenizer ) create_api ( classifier , run = True , api_type = \"text-classification\" ) You can open http://127.0.0.1:8000/docs Swagger UI in your browser to test the API \ud83d\udd25 Image Classification API from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) Open in your browser and try out the API. You can upload any image to try. Preview Create Interactive UI with Gradio To get a full list of available api_types for GradioApp you can call chitra.serve.GradioApp.get_available_api_types() . Image Classification Demo Instantiate ImageNet pretrained Model with Tensorflow import tensorflow as tf from chitra.core import load_imagenet_labels image_shape = ( 224 , 224 ) model = tf . keras . applications . MobileNetV2 ( weights = \"imagenet\" ) IMAGENET_LABELS = load_imagenet_labels () Chitra will automatically create a preprocessing function based on api_type . But if you want to override and define your own then you can just pass any callable function. def postprocess ( preds ): preds = tf . argmax ( preds , 1 ) . numpy () label = IMAGENET_LABELS [ preds [ 0 ]] return label Create GradioApp with Chitra from chitra.serve.app import GradioApp app = GradioApp ( \"image-classification\" , model = model , image_shape = image_shape , postprocess_fn = postprocess , ) If you want to share the live internet url then set share=True , it will create a public url that you can share with anyone over the internet. app . run ( share = True ) Preview","title":"Serve"},{"location":"source/api/serve/model_server/#serving-ml-models-with-api-or-ui-app","text":"Create Rest API or Interactive UI app for Any Learning Model - ML, DL, Image Classification, NLP, Tensorflow, PyTorch or SKLearn.","title":"Serving ML Models with API or UI app"},{"location":"source/api/serve/model_server/#what-can-it-do","text":"Create Rest API endpoint for Model Serving Create Interactive UI for Model Prototype Demo Share UI Demo with everyone by generating public url Predefined processing functions for image classification (NLP processing functions coming soon) Override custom preprocessing and Postprocessing function with your own. Request Response Schema (JSON body) will be changed based on the api_type . install: pip install -U \"chitra[serve]\" Default available API types are: Image Classification Object Detection Text Classification Question Answering To get a full list of available API types you can call chitra.serve.API.get_available_api_types() .","title":"What can it do?"},{"location":"source/api/serve/model_server/#create-rest-api","text":"","title":"Create Rest API"},{"location":"source/api/serve/model_server/#text-classification-api","text":"You can easily create Sentiment Analysis API. In this example, I will use HuggingFace to load the Sentiment Analysis Model but feel free to use other models as well. from transformers import AutoModelForSequenceClassification , AutoTokenizer , pipeline from chitra.serve import create_api tokenizer = AutoTokenizer . from_pretrained ( \"finiteautomata/beto-sentiment-analysis\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"finiteautomata/beto-sentiment-analysis\" ) classifier = pipeline ( \"sentiment-analysis\" , model = model , tokenizer = tokenizer ) create_api ( classifier , run = True , api_type = \"text-classification\" ) You can open http://127.0.0.1:8000/docs Swagger UI in your browser to test the API \ud83d\udd25","title":"Text Classification API"},{"location":"source/api/serve/model_server/#image-classification-api","text":"from chitra.serve import create_api from chitra.trainer import create_cnn model = create_cnn ( 'mobilenetv2' , num_classes = 2 ) create_api ( model , run = True , api_type = 'image-classification' ) Open in your browser and try out the API. You can upload any image to try.","title":"Image Classification API"},{"location":"source/api/serve/model_server/#preview","text":"","title":"Preview"},{"location":"source/api/serve/model_server/#create-interactive-ui-with-gradio","text":"To get a full list of available api_types for GradioApp you can call chitra.serve.GradioApp.get_available_api_types() .","title":"Create Interactive UI with Gradio"},{"location":"source/api/serve/model_server/#image-classification-demo","text":"Instantiate ImageNet pretrained Model with Tensorflow import tensorflow as tf from chitra.core import load_imagenet_labels image_shape = ( 224 , 224 ) model = tf . keras . applications . MobileNetV2 ( weights = \"imagenet\" ) IMAGENET_LABELS = load_imagenet_labels () Chitra will automatically create a preprocessing function based on api_type . But if you want to override and define your own then you can just pass any callable function. def postprocess ( preds ): preds = tf . argmax ( preds , 1 ) . numpy () label = IMAGENET_LABELS [ preds [ 0 ]] return label Create GradioApp with Chitra from chitra.serve.app import GradioApp app = GradioApp ( \"image-classification\" , model = model , image_shape = image_shape , postprocess_fn = postprocess , ) If you want to share the live internet url then set share=True , it will create a public url that you can share with anyone over the internet. app . run ( share = True )","title":"Image Classification Demo"},{"location":"source/api/serve/model_server/#preview_1","text":"","title":"Preview"},{"location":"source/api/visualization/metrics/","text":"Visualizing Metrics Plot Confusion Matrix from chitra.visualization.metrics import plot_confusion_matrix y_pred = [ 1 , 1 , 0 , 1 ] y_true = [ 0 , 1 , 0 , 1 ] display_labels = ( 'class A' , 'class B' ) plot_confusion_matrix ( y_pred , y_true , display_labels = display_labels )","title":"Visualization"},{"location":"source/api/visualization/metrics/#visualizing-metrics","text":"","title":"Visualizing Metrics"},{"location":"source/api/visualization/metrics/#plot-confusion-matrix","text":"from chitra.visualization.metrics import plot_confusion_matrix y_pred = [ 1 , 1 , 0 , 1 ] y_true = [ 0 , 1 , 0 , 1 ] display_labels = ( 'class A' , 'class B' ) plot_confusion_matrix ( y_pred , y_true , display_labels = display_labels )","title":"Plot Confusion Matrix"},{"location":"source/cli/builder/builder-create/","text":"Automatic Docker Image Creation for Any ML/DL Model \ud83d\udc33 chitra CLI can build docker image for any kind of Machine Learning or Deep Learning Model. You need to create a main.py file which will contain an object of type chitra.serve.ModelServer and its name should be app . If you have any external Python dependency then create a requirements.txt file and keep in the same directory. If the above conditions are satisfied then just run chitra builder run --path MAIN_FILEPATH Usage chitra builder create [OPTIONS] Options: --path TEXT [default: ./] --port TEXT --tag TEXT --help Show this message and exit. path is the file location where main.py and requirements.txt is present. You can specify which port to run your app on. By default, it is 8080. To set the tag of the docker image use tag argument. Example: Auto Build Docker Image for HuggingFace Text Classification Model API Create ModelServer First create Text Classification model from HuggingFace. chitra provides create_api method to create API for any kind of ML/DL model. We specify the api_type=\"text-classification\" and get an app object. from transformers import AutoModelForSequenceClassification , AutoTokenizer , pipeline from chitra.serve import create_api tokenizer = AutoTokenizer . from_pretrained ( \"microsoft/xtremedistil-l6-h256-uncased\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"microsoft/xtremedistil-l6-h256-uncased\" ) classifier = pipeline ( \"text-classification\" , model = model , tokenizer = tokenizer ) app = create_api ( classifier , run = False , api_type = \"text-classification\" ) . app Use Chitra CLI to auto-build Docker Image Go to terminal and change the directory to the path where main.py is present. Run chitra builder create --path ./FILEPATH --tag chitra-server . That's all you need to do! You can run the docker image using docker run -p 8080:8080 chitra-server from terminal.","title":"Auto Docker Building"},{"location":"source/cli/builder/builder-create/#automatic-docker-image-creation-for-any-mldl-model","text":"chitra CLI can build docker image for any kind of Machine Learning or Deep Learning Model. You need to create a main.py file which will contain an object of type chitra.serve.ModelServer and its name should be app . If you have any external Python dependency then create a requirements.txt file and keep in the same directory. If the above conditions are satisfied then just run chitra builder run --path MAIN_FILEPATH","title":"Automatic Docker Image Creation for Any ML/DL Model \ud83d\udc33"},{"location":"source/cli/builder/builder-create/#usage","text":"chitra builder create [OPTIONS] Options: --path TEXT [default: ./] --port TEXT --tag TEXT --help Show this message and exit. path is the file location where main.py and requirements.txt is present. You can specify which port to run your app on. By default, it is 8080. To set the tag of the docker image use tag argument.","title":"Usage"},{"location":"source/cli/builder/builder-create/#example-auto-build-docker-image-for-huggingface-text-classification-model-api","text":"","title":"Example: Auto Build Docker Image for HuggingFace Text Classification Model API"},{"location":"source/cli/builder/builder-create/#create-modelserver","text":"First create Text Classification model from HuggingFace. chitra provides create_api method to create API for any kind of ML/DL model. We specify the api_type=\"text-classification\" and get an app object. from transformers import AutoModelForSequenceClassification , AutoTokenizer , pipeline from chitra.serve import create_api tokenizer = AutoTokenizer . from_pretrained ( \"microsoft/xtremedistil-l6-h256-uncased\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"microsoft/xtremedistil-l6-h256-uncased\" ) classifier = pipeline ( \"text-classification\" , model = model , tokenizer = tokenizer ) app = create_api ( classifier , run = False , api_type = \"text-classification\" ) . app","title":"Create ModelServer"},{"location":"source/cli/builder/builder-create/#use-chitra-cli-to-auto-build-docker-image","text":"Go to terminal and change the directory to the path where main.py is present. Run chitra builder create --path ./FILEPATH --tag chitra-server . That's all you need to do! You can run the docker image using docker run -p 8080:8080 chitra-server from terminal.","title":"Use Chitra CLI to auto-build Docker Image"}]}