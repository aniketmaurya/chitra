{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datagenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datagenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from chitra.utils import gpu_dynamic_mem_growth\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "gpu_dynamic_mem_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from chitra.image import read_image, resize_image\n",
    "from typeguard import check_argument_types, typechecked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator\n",
    "## components\n",
    "> components are methods that can be easily overridden\n",
    "- image path gen\n",
    "- image label gen\n",
    "- image resizer\n",
    "\n",
    "\n",
    "> the generator object will also support  callbacks that can update the components \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def benchmark(dataset, num_epochs=2, fake_infer_time=0.001):\n",
    "    \"\"\"Use this function to benchmark your Dataset loading time\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(fake_infer_time)\n",
    "    tf.print(\n",
    "        f\"Execution time for {num_epochs} epochs: {time.perf_counter() - start_time :0.3f} seconds\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_filenames(root_dir):\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    return glob(str(root_dir / \"*/*\"))\n",
    "\n",
    "\n",
    "def get_label(filename):\n",
    "    return filename.split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageSizeList:\n",
    "    def __init__(self, img_sz_list=None):\n",
    "\n",
    "        if isinstance(img_sz_list, (list, tuple)):\n",
    "            if len(img_sz_list) != 0 and not isinstance(img_sz_list[0], (list, tuple)):\n",
    "                img_sz_list = [img_sz_list][:]\n",
    "\n",
    "        self.start_size = None\n",
    "        self.last_size = None\n",
    "        self.curr_size = None\n",
    "        self.img_sz_list = img_sz_list\n",
    "\n",
    "        try:\n",
    "            self.start_size = img_sz_list[0]\n",
    "            self.last_size = img_sz_list[-1]\n",
    "            self.curr_size = img_sz_list[0]\n",
    "        except (IndexError, TypeError) as e:\n",
    "            print(\"No item present in the image size list\")\n",
    "            self.curr_size = None  # no item present in the list\n",
    "\n",
    "    def get_size(self):\n",
    "        img_sz_list = self.img_sz_list\n",
    "        try:\n",
    "            self.curr_size = img_sz_list.pop(0)\n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Returning the last set size which is: {self.curr_size}\")\n",
    "\n",
    "        return self.curr_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz_list = ImageSizeList(None)\n",
    "img_sz_list.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels)}\n",
    "\n",
    "    def encode(self, label):\n",
    "        return self.label_to_idx[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Pipeline:\n",
    "    @typechecked\n",
    "    def __init__(self, funcs: Union[Callable, list, tuple] = []):\n",
    "        assert check_argument_types()\n",
    "        if isinstance(funcs, list):\n",
    "            self.funcs = funcs\n",
    "        elif callable(funcs):\n",
    "            self.funcs = [funcs]\n",
    "        else:\n",
    "            self.funcs = []\n",
    "\n",
    "    @typechecked\n",
    "    def add(self, func: Callable):\n",
    "        assert check_argument_types()\n",
    "        self.funcs.append(func)\n",
    "\n",
    "    def __call__(self, item):\n",
    "        try:\n",
    "            for func in self.funcs:\n",
    "                item = func(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while applying function in pipeline!\")\n",
    "            raise e\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Dataset:\n",
    "    MAPPINGS = {\n",
    "        \"PY_TO_TF\": {\n",
    "            str: tf.string,\n",
    "            int: tf.int32,\n",
    "            float: tf.float32\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dir: Union[str, Path],\n",
    "        image_size=[],\n",
    "        transforms=None,\n",
    "        default_encode=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        train_dir(str): Path for training data\n",
    "        \"\"\"\n",
    "        self.get_filenames = get_filenames\n",
    "        self.read_image = read_image\n",
    "        self.get_label = get_label\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.root_dir = train_dir\n",
    "        self.default_encode = default_encode\n",
    "        self.filenames = self.get_filenames(train_dir)\n",
    "        self.num_files = len(self.filenames)\n",
    "        self.image_size = image_size\n",
    "        self.img_sz_list = ImageSizeList(self.image_size[:])\n",
    "\n",
    "        self.labels = kwargs.get(\"labels\", self.get_labels())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def _process(self, filename):\n",
    "        image = self.read_image(filename)\n",
    "        label = self.get_label(filename)\n",
    "        return image, label\n",
    "\n",
    "    def _reload(self):\n",
    "        self.filenames = self.get_filenames(self.root_dir)\n",
    "        self.num_files = len(self.filenames)\n",
    "        self.img_sz_list = ImageSizeList(None or self.image_size[:])\n",
    "        self.labels = self.get_labels()\n",
    "\n",
    "    def _capture_return_types(self):\n",
    "        return_types = []\n",
    "        for e in self.generator():\n",
    "            outputs = e\n",
    "            break\n",
    "        if isinstance(outputs, tuple):\n",
    "            for ret_type in outputs:\n",
    "                return_types.append(ret_type.dtype if tf.is_tensor(\n",
    "                    ret_type) else Dataset.MAPPINGS[\"PY_TO_TF\"][type(ret_type)])\n",
    "        else:\n",
    "            return_types.append(ret_type.dtype if tf.is_tensor(ret_type) else\n",
    "                                Dataset.MAPPINGS[\"PY_TO_TF\"][type(ret_type)])\n",
    "        return tuple(return_types)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        return self._process(filename)\n",
    "\n",
    "    def update_component(self, component_name, new_component, reload=True):\n",
    "        setattr(self, component_name, new_component)\n",
    "        print(f\"{component_name} updated with {new_component}\")\n",
    "        self._reload()\n",
    "\n",
    "    def get_labels(self):\n",
    "        # get labels should also update self.num_classes\n",
    "        root_dir = self.root_dir\n",
    "        labels = set()\n",
    "        folders = glob(f\"{root_dir}/*\")\n",
    "        for folder in folders:\n",
    "            labels.add(os.path.basename(folder))\n",
    "\n",
    "        labels = sorted(labels)\n",
    "        self.NUM_CLASSES = len(labels)\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def label_encoder(self, label):\n",
    "        idx = self.label_to_idx.get(label, None)\n",
    "        assert idx is not None, f\"Error while converting label={label} to index!\"\n",
    "        return idx\n",
    "\n",
    "    def generator(self, shuffle=False):\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        img_sz = self.img_sz_list.get_size()\n",
    "        n = len(self.filenames)\n",
    "        for i in range(n):\n",
    "            image, label = self.__getitem__(i)\n",
    "            if img_sz:\n",
    "                image = resize_image(image, img_sz)\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "            if self.default_encode is True:\n",
    "                label = self.label_encoder(label)\n",
    "            yield image, label\n",
    "\n",
    "    def get_tf_dataset(self, output_shape=None, shuffle=True):\n",
    "        return_types = self._capture_return_types()\n",
    "        self._reload()\n",
    "        generator = partial(self.generator, shuffle=shuffle)\n",
    "        datagen = tf.data.Dataset.from_generator(generator, return_types,\n",
    "                                                 output_shape)\n",
    "\n",
    "        return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# data_path = '/data/aniket/tiny-imagenet/data/tiny-imagenet-200/train'\n",
    "cat_dog_path = \"/Users/aniket/Pictures/data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(cat_dog_path, image_size=[(28, 28), (32, 32), (64, 64)])\n",
    "# ds = Dataset('/data/aniket/tiny-imagenet/data/tiny-imagenet-200/train')\n",
    "# ds = Dataset('/data/aniket/tiny-imagenet/data/tiny-imagenet-200/train', image_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_files(path):\n",
    "#     return glob(f'{path}/*/images/*')\n",
    "\n",
    "# def get_label(path):\n",
    "#     return path.split('/')[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.update_component('get_filenames', load_files)\n",
    "# ds.update_component('get_label', get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ds.generator(True):\n",
    "    print(e[0].dtype, e[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = ds.get_tf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in dl.take(1):\n",
    "    print(e[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev.export import notebook2script;notebook2script('03_datagenerator.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
