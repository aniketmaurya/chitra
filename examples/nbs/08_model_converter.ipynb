{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp converter.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-cartoon",
   "metadata": {},
   "source": [
    "# Model Interconversion\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from chitra.utility.import_utils import INSTALLED_MODULES, is_installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "def pytorch_to_onnx(model, tensor, export_path=\"temp.onnx\"):\n",
    "    # Input to the model\n",
    "    torch_out = model(tensor)\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(\n",
    "        model,  # model being run\n",
    "        tensor,  # model input (or a tuple for multiple inputs)\n",
    "        export_path,  # where to save the model (can be a file or file-like object)\n",
    "        export_params=True,  # store the trained parameter weights inside the model file\n",
    "        opset_version=10,  # the ONNX version to export the model to\n",
    "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "        input_names=[\"input\"],  # the model's input names\n",
    "        output_names=[\"output\"],  # the model's output names\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\"},  # variable length axes\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "    return export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import onnx\n",
    "import tf2onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "\n",
    "def onnx_to_pytorch(onnx_model):\n",
    "    if isinstance(onnx_model, str):\n",
    "        onnx_model = onnx.load(onnx_model)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    pytorch_model = ConvertModel(onnx_model)\n",
    "    return pytorch_model\n",
    "\n",
    "\n",
    "def tf2_to_onnx(model, opset=None, output_path=None, **kwargs):\n",
    "    inputs_as_nchw = kwargs.get(\"inputs_as_nchw\", \"input0:0\")\n",
    "    onnx_model = tf2onnx.convert.from_keras(\n",
    "        model, opset=opset, output_path=output_path, inputs_as_nchw=inputs_as_nchw\n",
    "    )\n",
    "    return onnx_model\n",
    "\n",
    "\n",
    "def tf2_to_pytorch(model, opset=None, **kwargs):\n",
    "    with tempfile.NamedTemporaryFile(mode='w') as fw:\n",
    "        filename = fw.name\n",
    "        onnx_model = tf2_to_onnx(tf_model, opset, output_path=filename, **kwargs)\n",
    "        fw.seek(0)\n",
    "        torch_model = onnx_to_pytorch(filename)\n",
    "    return torch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-plenty",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "model1 = timm.create_model(\"resnet18\")\n",
    "model1.eval()\n",
    "\n",
    "model_inter_path = pytorch_to_onnx(model1, torch.randn(1, 3, 224, 224))\n",
    "model2 = onnx_to_pytorch(model_inter_path)\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "np.allclose(model1(x).detach().numpy(), model2(x).detach().numpy(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model = tf.keras.applications.MobileNetV2()\n",
    "# model_test = tf2_to_pytorch(tf_model, inputs_as_nchw=None, opset=13).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-senior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-class",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chitra.image import Chitra\n",
    "\n",
    "image = Chitra(\"https://c.files.bbci.co.uk/957C/production/_111686283_pic1.png\")\n",
    "image.image = image.image.resize((224, 224)).convert(\"RGB\")\n",
    "image.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.cast(image.to_tensor(\"tf\"), tf.float32) / 127.5 - 1.0\n",
    "x1 = tf.expand_dims(x1, 0)\n",
    "\n",
    "x2 = image.numpy()[:].astype(np.float32) / 255\n",
    "x2 = np.expand_dims(x2, 0)\n",
    "x2 = torch.from_numpy(x2)\n",
    "x2 = x2.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chitra(((x1[0] + 1) * 127.5).numpy().astype(\"uint8\")).imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chitra.core import IMAGENET_LABELS\n",
    "\n",
    "res1 = tf.math.softmax(tf_model.predict(x1), 1)\n",
    "IMAGENET_LABELS[tf.argmax(res1, 1).numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = my_model(x2)\n",
    "# IMAGENET_LABELS[torch.argmax(res2).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-watch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.shape, res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-literature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-education",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
